\documentclass[11pt]{article}

\usepackage{fullpage}

\input{commands}

\newcommand{\mysection}[1]{\vspace{3mm}\noindent\textbf{#1} \\}
\DeclareMathOperator*{\argmax}{\mathsf{argmax}}

\title{Notes}

\begin{document}

\date{}

\maketitle

\vspace{-2cm}


\section{Model decision over FHE}

Problem: Two parties have private data and want to learn the result of a function on their data.

\noindent Example use case:  A hospital  has some private medical records of patients and a client wants to learn whether he is likely to get some disease or to get some good treatment from a doctor based on that data. Neither the hospital nor the client want to share their private data.

Discussion:
We realized that using FHE for running the machine learning algorithm is not that useful because the hospital can learn on plaintext data. Instead, what is more interesting is to hide the resulting learned model from the patient (another example is Google flu trends because Google does not want to release its parameters). So we want to perform FHE on the decision algorithm that uses the model to predict the outcome, and even this was non-trivial to figure out.
Using FHE here makes sense because we are going to use it for security benefits rather than performance benefits.


\subsection{Our solution for hyperplane decision surfaces}

\newcommand{\res}{\mathsf{res}}

We assume a class of classification models parameterized by a hyperplane $w$
for which the decision function can be expressed as:
\begin{equation*}
  \hat{y}(x) = \text{sgn}(\langle w, x \rangle) 
\end{equation*}

In other words, the server has a vector $w$ (the model) and the client has a
vector $x$ (a particular feature vector). The client wants to know whether
$\langle x, w \rangle > 0$, where $\langle \cdot, \cdot \rangle$ denotes dot product. For
now, we assume that $x$ has a finite dimensional basis (this excludes RBF
surfaces, for example).

The client provides $\enc(x)$ to the server, which can easily compute
$\enc(\langle x, w \rangle)$ by only using 1-depth FHE. 

Note that Paillier suffices for this computation. Even though Paillier does not have multiplication homomorphism, we can still implement
multiplication because the server knows $w$ (the model) in plaintext, and can simply raise a ciphertext to the power of $w_i$ and achieve multiplication by $w_i$. Moreover, Paillier has a large plaintext domain so it can certainly fit the ranges of the classification algorithm. 

Let $\res = \langle x,
w \rangle$.

The challenge is deciding if $\res > 0$. Simply giving the client $\enc(\res)$ leaks $\res$ to the client, which is more information about the model than the final decision.

Instead, the idea is for the server to choose a random $\epsilon \in \bbZ_q$ where $q$ is the space of the plaintext in FHE. The server computes $\enc(\epsilon + \res)$ and the client is able to decrypt this ciphertext and obtain $\epsilon + \res \mod q$, which information-theoretically leaks nothing about $\res$.

Now, ideally we would like the client to compare $\epsilon + \res$ to $\epsilon + q/2$, because $\res > q/2$ indicates a negative $\res$ (that wrapped around). We can use Yao's comparison protocol (the millionaires problem -- a simple one interaction protocol) to decide the result of this comparison without leaking any private information. The challenge though is that the client has $\epsilon + \res \mod q$, and $\mod q$ affects correctness.

The idea is to use a new experiment that has a chance of outputting the correct result $>1/2$, and to repeat this protocol a few times and take majority in order to amplify correctness (which increases exponentially in the number of repeats):

\newcommand{\eps}{\epsilon}

\begin{enumerate}
\item
    The server chooses $\epsilon \in \bbZ_q$ at random. If $\epsilon \geq q/2$,
    let $v = \eps - q/2$. The intuition is that an $\eps \geq q/2$ is likely to
    wrap around when added to another number. If $\eps < q/2$, let $v = \eps +
    q/2$.
\item Run Yao's protocol, where the client's input is $\eps + \res \mod q$ and the server's input is $v$.
\end{enumerate}

We now analyze this protocol to show that the probability of success is greater
than half. Let $r \in \bbZ_q$, and $\epsilon \in \bbZ_q$ be drawn uniformly at
random. We also assume that all $r \in \bbZ_q$ are equally likely.
\rnote{We don't need to assume that r is equally likely. Our correctness holds for all $r$. If we assume all r-s are equally likely, we only prove that our protocol succeeds on barely more than half the possible results, but we can in fact show that it should succeed for all possible results with high probability.}

 For now, we
restrict ourselves to odd $q$. Denote $\bbZ_q^+$ as the integers in $[0,
\frac{q-1}{2}]$, and $\bbZ_q^-$ as the integers in $[\frac{q-1}{2}+1, q)$.

\rnote{once you fix an $r$, you should be able to write the probabilities below in terms of $q-r$ or $r$, for the case when $r>q/2$ and $r<q/2$. }
That means $r \in \bbZ_q^+$ we will consider to encode a number $\geq 0$, and
$r \in \bbZ_q^-$ we will consider to encode a number $< 0$. Let $v$ be defined
as before. We want to show that:
\begin{align*}
    \Pr\left[ (r+\epsilon) \text{ mod } q < v\right]
    \begin{cases}
        > 1/2 & \mbox{if } r\in\bbZ_q^+  \\
        < 1/2 & \mbox{if } r\in\bbZ_q^-
    \end{cases}
\end{align*}
We begin by showing that if $r \in \bbZ_q^+$ and $q \geq 7$, then
$\Pr\left[(r+\epsilon)\text{ mod }q<v\right] > 1/2$:
\begin{align*}
    \Pr\left[(r+\epsilon)\text{ mod }q<v : r \in \bbZ_q^+ \right] &=
        \frac{1}{q}\sum\limits_{i=0}^{q-1}\Pr[(r+\epsilon)\text{ mod }q<v : \epsilon=i, r\in \bbZ_q^+ ] \\
        &= \frac{1}{q}\left[ \sum\limits_{i=0}^{(q-1)/2-1} \frac{(q-1)/2}{(q-1)/2+1} + \sum\limits_{i=(q-1)/2}^{q-1} \frac{i-(q-1)/2}{(q-1)/2+1} \right] \\
        &= \frac{1}{4}\left[3+\frac{1}{q}-\frac{8}{1+q}\right]
\end{align*}


\rnote{the probability statements above and below should be over the random choice of $\epsilon$ rather than $r$}

Notice that this quantity is always $> 1/2$ if $q \geq 7$. We now show the other side, that
if $r \in \bbZ_q^-$, then $\Pr\left[(r+\epsilon)\text{ mod }q<v\right] < 1/2$:
\begin{align*}
    \Pr\left[(r+\epsilon)\text{ mod }q<v:r\in\bbZ_q^-\right] &=
        \frac{1}{q}\sum\limits_{i=0}^{q-1}\Pr[(r+\epsilon)\text{ mod }q<v : \epsilon=i,r\in\bbZ_q^-] \\
        &= \frac{1}{q}\left[ \sum\limits_{i=0}^{(q-1)/2-1} \frac{i}{(q-1)/2} + \sum\limits_{i=(q-1)/2}^{q-1} 0 \right] \\
        &= \frac{q-3}{4q}
\end{align*}
It is clear that for $q \geq 3$, this quantity is $< 1/2$. Since we already
require $q \geq 7$ above, this is ok.

\rnote{as a small nit for style, I (and cryptographers) find this notation more clear : $\Pr[\eps \gets \bbZ_q: (r+\eps) \mod q < v]$ because it is clearer where the randomness comes from and what is the order of steps in the experiment.}

\rnote{based on this probability bound, would be good to estimate how many repetitions we need, for example, using Hoeffding's inequality -- see end of Wikipedia page -- or other inequality you prefer}

%if r > q/2
 %if e > q/2 : correct result always
 %if e < q/2 for  q - r < e < q/2 incorrect result -- but for e < q-r it is correct
%each case for e happens with chance 1/2

%if r < q/2 :
%if e < q/2 : correct result always
%if e > q/2: correct result sometimes


\subsection{Na\"ive Bayes solution}

%For na\"ive Bayes, we need to convert the table of values into a matrix so we can FHE multiply it to the input $x$. TODO: we need to figure out how to compute the maximum of $k$ values over FHE, likely using the idea above which computes $\max$ of $q/2$ and $\res$.
%
\newcommand{\NB}{Na\"ive Bayes}

We now consider categorial \NB{}. Suppose we have $k$ classes and $n$ features.
Then recall that a \NB{} classifier is parameterized by $\theta_{\mathcal{C}} =
\{\theta_{c}=p(\mathcal{C}=c)\}_{c=1}^{k}$ and $\theta_{\mathcal{F}|c} =
\{\theta_{f|c}= \{ p(\mathcal{F}_f=x|c) \}_{x=1}^{|\mathcal{F}_f|}
\}_{f=1}^{n}$ for each $c = 1,...,k$. The decision function is simply:
\begin{equation*}
  \hat{y}(x) = \argmax_{c} p(c, x) = p(c)\prod\limits_{i=1}^{n}p(x_i|c)
\end{equation*}
To make this easier for FHE eval, we assume that the client supplies us with each feature value
in $1$-of-$k$ encoding, so we receive $n$ ciphertexts $(\enc(x_1), ..., \enc(x_n))$. So now we
can express the decision function as a product of dot products:
\begin{equation*}
  \hat{y}(x) = \argmax_{c} p(c, x) = p(c)\prod\limits_{i=1}^{n} \langle \theta_{\mathcal{F}_i|c}, x_i \rangle
\end{equation*}

$\max$ is tricky and we currently do not know how to do it.

\newpage



\bibliography{main}
\bibliographystyle{alpha}





\end{document}
